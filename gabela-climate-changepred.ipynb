{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of Contents  \n<a id='toc'></a>\n1. [Importing and Installing Libraries](#libraries)  \n\n\n\n2. [Load Training Data](#train)  \n    \n\n\n3. [Load Testing Data](#test)   \n     \n\n\n4. [Text Preprocessing](#cleaning)\n    \n\n\n5. [Exploratory Data Analysis](#eda) \n\n       \n\n\n6. [Feature Extraction](#vect)\n    \n\n\n\n7. [Model Training](#modelling)\n\n    \n\n\n8. [Test Data](#tes)\n  \n\n\n\n9. [Conclusion](#conclude)\n\n\n\n","metadata":{}},{"cell_type":"code","source":"!pip install plotly","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install stopwordsiso","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install yellowbrick","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install imblearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordcloud","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall sklearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall scikit-learn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sklearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Importing and Installing Libraries \n<a id='libraries'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"code","source":"# Libraries used to load dataframe and visualize data\nimport numpy as np \nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.features import RadViz\nfrom wordcloud import WordCloud\nimport plotly.io as pio\npio.renderers.default='notebook'\n%matplotlib inline\n\n# Noise removal helper libraries\nimport re\nimport string \nfrom stopwordsiso import stopwords as sw\nfrom nltk.corpus import stopwords\n\n# Text Preprocessing\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n#import nltk \n#nltk.download()\n\n# Feature Engineering and Data preparation for modelling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n# Model building and training\nfrom sklearn.svm import SVC \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Model evaluation\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\n\n#save the final model and vectorizer\nimport pickle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Load Training Data\n<a id='train'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"code","source":"# Load the training dataset\ntrain = pd.read_csv(r\"train.csv\")\n\n# Display the first 10 rows training dataset dataframe\ntrain.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Load Testing Data\n<a id='test'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"code","source":"# Load the testing dataset\ntest = pd.read_csv(r\"test_with_no_labels.csv\")\n\n# Display the first 10 rows\ntest.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Text Preprocessing\n<a id='cleaning'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"markdown","source":"### 4.1. Noise Removal","metadata":{}},{"cell_type":"code","source":"# Create dataframe cleaning function\ndef text_preprocessing (df):\n    \n    # Regex pattern for url addresses\n    pattern_url = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)' \n    \n    # Regex patter for numerical values\n    pattern_digits = r'\\d+'\n    \n    # Regex pattern for twitter handles\n    pattern_handles = r'@(\\w+)'\n    \n    # Create a copy of the original dataframe to perform cleaning on\n    df = df.copy()\n    \n    # Use the url regex pattern  to remove website addresses\n    df['message'] = df['message'].replace(to_replace = pattern_url, value = '', regex = True)\n    \n    # Use numerical values regex pattern to remove digits \n    df['message'] = df['message'].replace(to_replace = pattern_digits, value = '', regex = True)\n    \n    # Use the twitter handle regex pattern to remove handles\n    df['message'] = df['message'].replace(to_replace = pattern_handles, value = '', regex = True)\n    \n    # Create a function low that converts text to lowercase\n    low = lambda tweets: ''.join([tweet.lower() for tweet in tweets])\n    df['message'] = df['message'].apply(low) # The low function is used convert values in the message column to lowercase\n    \n    # Create a function punct that removes punctuation from a dataframe\n    punct = lambda tweets: ''.join([tweet for tweet in tweets if tweet not in string.punctuation])\n    df['message'] = df['message'].apply(punct)# The punct function is used ot remove punctuation from the message column\n    \n    # Use a regex pattern to remove non-ascii characters from the message column in the dataframe\n    df.message.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n    \n    #return the final dataframe object which has undergone noise removal\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call dataframe cleaning function with the testing dataset as input, creating a new, clean training dataset dataframe\nclean_train = text_preprocessing(train)\n\n# Display the first 10 row of the clean training dataframe, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(clean_train.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the dataframe cleaning function with the testing dataset as input, creating a new, clean testing dataset dataframe\nclean_test = text_preprocessing(test)\n\n# Display the first 10 rows of the clean testing dataframe, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(clean_test.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Tokenisation and Stopwords Removal","metadata":{}},{"cell_type":"code","source":"# Create function that tokenizes the words in a dataframe\ndef tokens(df, column_name):\n    \n    # Create a copy of the dataframe to perform tokenization on\n    df = df.copy()\n    \n    # Instantiate TweetTokenizer, as tknzr, with reduce_len to remove redundant lettering in words\n    # e.g 'yessss' transormed to 'yes'\n    tknzr = TweetTokenizer(reduce_len = True)\n    \n    # Format the message column using TweetTokenizer\n    df[column_name] = df[column_name].apply(tknzr.tokenize)\n    \n    # Return a dataframe object that has been tokenized\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the tokens function on the clean training dataset dataframe and message column, \n# creating a new tokenized training dataframe\ntokens_train = tokens(clean_train, 'message')\n\n# Display the first 10 row in the tokenized training dataset dataframe, allowing for maximum width in the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(tokens_train.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the tokens function on the clean testing dataset dataframe and message column, creating a new tokenized testing dataframe\ntokens_test = tokens(clean_test, 'message')\n\n# Display the first 10 rows of the tokenized testing dataset dataframe, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(tokens_test.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function that removes stop words and 'rt' (retweet) from a dataframe column\ndef stop(df, column_name):\n    \n    # Create a copy of the dataframe stopwords removal is performed on\n    df = df.copy()\n    \n    # Create a function, rt, that returns tokenized words which are not 'rt'\n    rt = lambda tweets: [tweet for tweet in tweets if tweet != 'rt']\n    \n    # Use the rt function on the specified dataframe column, removing all 'rt'instances from each observation\n    df[column_name] = df[column_name].apply(rt)\n    \n    #Create a function stops which returns the words in a tokenized dataframe that do not appear in a stopwords set\n    stops = lambda tweets: [tweet for tweet in tweets if tweet not in sw('en')]\n    \n    # Use the stops function on the specified dataframe column, removing all stopwords\n    df[column_name] = df[column_name].apply(stops)\n    \n    #Return the dataframe without rts and stopwords\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the stop function on the tokenized training dataset dataframe, creating a new training dataframe without stopwords and rts\nstops_train = stop(tokens_train, 'message')\n\n# Display the first 10 rows of the training dataframe without stopwords and rts, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(stops_train.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the stops function the tokenized testing dataset dataframe, creating a new testing dataframe without stopwords and rts\nstops_test = stop(tokens_test, 'message')\n\n# Display the first 10 rows of the testing dataframe without stopwords and rts, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(stops_test.head(10))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3. Lemmatizaton","metadata":{}},{"cell_type":"code","source":"# Instantiate WordNetLemmatizer as lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Create a new dataframe to be lemmatized using the stops_train dataframe\nlemmatized_train = stops_train.copy()\n\n# Use lemmatizer on the message column of the lemmatized_train dataframe\nlemmatized_train['message'] = stops_train['message'].apply(lambda sentence : [lemmatizer.lemmatize(word) for word in sentence])\n\n# Display the first 10 rows of the lemmatized_train dataframe, allowing maxmimum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(lemmatized_train.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new dataframe to be lemmatized using th stops_test dataframe\nlemmatized_test = stops_test.copy()\n\n# Use lemmatizer on the message column of the lemmatized_test dataframe\nlemmatized_test['message'] = stops_test['message'].apply(lambda sentence: [lemmatizer.lemmatize(word) for word in sentence])\n\n# Display the first 10 rows of the lemmatized_test dataframe, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(lemmatized_test.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join the tokenized words in the lemmatized_train message column into sentences\nlemmatized_train['message'] = [' '.join(tweet) for tweet in lemmatized_train['message'].values]\n\n# Display the first 10 rows of the lemmatized_train dataframe, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(lemmatized_train.head(10))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join the tokenized words in the lemmatized_test message column into sentence \nlemmatized_test['message'] = [' '.join(tweet) for tweet in lemmatized_test['message'].values]\n\n# Display the first 10 rows of the lemmatized_test dataframe, allowing maximum width for the message column\nwith pd.option_context('display.max_colwidth', 400):\n    display(lemmatized_test.head(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Exploratory Data Analysis\n<a id='eda'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"markdown","source":"### 5.1. Label Analysis","metadata":{}},{"cell_type":"code","source":"# Create a new dataframe by grouping tweets by sentiment and counting the number of tweets in each sentiment\ngroup = lemmatized_train.groupby('sentiment').count()['message'].reset_index().sort_values(by = 'message', ascending = False)\n\n# Display the new dataframe.  The sentiment column displays the four classes, \n# while the message column display the number of tweets per class\ngroup","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bar Plot","metadata":{}},{"cell_type":"code","source":"# Create a bar plot using the group dataframe  to visualise the number of tweets per class\nfig = go.Figure(go.Bar(x = ['Positive', 'News', 'Neutral', 'Negative'],y = group['message'], # Specify x and y variables\n                       marker = {'color': group['message'],'colorscale': 'plasma'})) # Select a colour for the graph\n\n# Add title, x and y axis labels to the bar chart\nfig.update_layout(yaxis_title = 'Tweets', xaxis_title = 'Sentiment', title = 'Number of Tweets Per Sentiment')\n\n# Show the bar plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Funnel Chart","metadata":{}},{"cell_type":"code","source":"# Create a funnel chart indicating the numbers of tweets per class as a proportion\nfig = go.Figure(go.Funnelarea(text = ['Positive', 'News', 'Neutral', 'Negative'], values = group['message'], \n                              marker = {'colors': group['message']}, # column to colour chart on\n                              title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}, # Chart Title\n                              labels = ['Positive', 'News', 'Neutral', 'Negative'])) # Legend labels\n\n# Show the funnel chart\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### WordClouds","metadata":{}},{"cell_type":"code","source":"# Collect tweets from the lemmatized_train dataframe written by pro man-made climate change individuals\npositive_words = \" \".join([sentence for sentence in lemmatized_train['message'][lemmatized_train['sentiment'] == 1]])\n\n# Instantiate  the wordcloud and use positive_words to generate a wordcloud for the 'pro' sentiment\npositive_wordcloud = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                               background_color = 'white').generate(positive_words)\n\n# Plot the positive wordcloud on an empty axis and use plt.show() to display it\nax1 = plt.imshow(positive_wordcloud)\nplt.axis('off')\nplt.show()\n\n# Collect tweets from the lemmatized_train dataframe written by anti man-made climate change individuals\nnegative_words = \" \".join([sentence for sentence in lemmatized_train['message'][lemmatized_train['sentiment'] == -1]])\n\n# Instantiate the wordcloud  and use negative_words to generate a wordcloud for the anti sentiment\nnegative_wordcloud = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                               background_color = 'white').generate(negative_words)\n\n# Plot the negative wordcloud on an empty axis and use plt.show() to display it\nax2 = plt.imshow(negative_wordcloud)\nplt.axis('off')\nplt.show()\n\n# Collect tweets from the lemmatized_train dataframe written by individuals with neutral views on man-made climate change\nneutral_words = \" \".join([sentence for sentence in lemmatized_train['message'][lemmatized_train['sentiment'] == 0]])\n\n# Instantiate the wordcloud and use neutral words to generate a wordcloud for the neutral sentiment\nneutral_wordcloud = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                              background_color = 'white').generate(neutral_words)\n\n# Plot the neutral wordcloud on an empty axis and use plt.show() to display it\nax3 = plt.imshow(neutral_wordcloud)\nplt.axis('off')\nplt.show()\n\n# Collect tweets from the lemmatized_train dataframe that link to actual news stories related to climate change\nnews_words = \" \".join([sentence for sentence in lemmatized_train['message'][lemmatized_train['sentiment'] == 2]])\n\n# Instantiate wordcloud and use news_words to generate a wordcloud for the news\nnews_wordcloud = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                           background_color = 'white').generate(news_words)\n\n# Plot the news wordcloud on an empty axis and use plt.show() to display it\nax4 = plt.imshow(news_wordcloud)\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of the four most frequent words in the wordclouds\nclimate_list = ['climate', 'change', 'global', 'warming']\n\n# Collect tweets from positive_words excluding the frequent words from the wordclouds\nnew_positive = \" \".join([word for word in positive_words.split() if word not in climate_list])\n\n# Instantiate  the wordcloud and use positive_words to generate a wordcloud for the 'pro' sentiment\npos_wc = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                   background_color = 'white').generate(new_positive)\n\n# Plot the positive wordcloud on an empty axis and use plt.show() to display it\nax1 = plt.imshow(pos_wc)\nplt.axis('off')\nplt.show()\n\n# Collect tweets from negative_words excluding the frequent words from the wordclouds\nnew_negative = \" \".join([word for word in negative_words.split() if word not in climate_list])\n\n# Instantiate the wordcloud  and use negative_words to generate a wordcloud for the anti sentiment\nneg_wc = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                               background_color = 'white').generate(new_negative)\n\n# Plot the negative wordcloud on an empty axis and use plt.show() to display it\nax2 = plt.imshow(neg_wc)\nplt.axis('off')\nplt.show()\n\n# Collect tweets from neutral_words excluding the frequent words from the wordclouds\nnew_neutral = \" \".join([word for word in neutral_words.split() if word not in climate_list])\n\n# Instantiate the wordcloud and use neutral words to generate a wordcloud for the neutral sentiment\nneu_wc = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                              background_color = 'white').generate(new_neutral)\n\n# Plot the neutral wordcloud on an empty axis and use plt.show() to display it\nax3 = plt.imshow(neu_wc)\nplt.axis('off')\nplt.show()\n\n# Collect tweets from news_words excluding the frequent words from the wordclouds\nnew_news = \" \".join([word for word in news_words.split() if word not in climate_list])\n\n# Instantiate wordcloud and use news_words to generate a wordcloud for the news\nnew_wc = WordCloud(width = 800, height = 500, random_state = 42, max_font_size = 100, \n                           background_color = 'white').generate(new_news)\n\n# Plot the news wordcloud on an empty axis and use plt.show() to display it\nax4 = plt.imshow(new_wc)\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Predictor Analysis","metadata":{}},{"cell_type":"markdown","source":"**Hashtags**","metadata":{}},{"cell_type":"code","source":"# Create function which extracts hashtags from the dataframe\ndef hashtag_extract(message):\n \n    # Create an empty list which will be used to collect hashtags\n    hashtags = []\n    \n    # For every word in message, find words that start with '#' and append them to the empty list\n    for tweet in message: # Look through message\n        ht = re.findall(r\"#(\\w+)\", tweet) # Use regex pattern to find hashtags\n        hashtags.append(ht) # Append the extracted hashtags to the empty list\n        \n    # Return the created list of hashtags\n    return hashtags","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the hashtags extract function to get hashtags associated with the positive class from the original training dataframe\npositive_hashtags = hashtag_extract(train['message'][train['sentiment'] == 1])\n\n# Use the hashtags extract function to get hashtags associated with the negative class from the original training dataframe\nnegative_hashtags = hashtag_extract(train['message'][train['sentiment'] == -1])\n\n# Use the hashtags extract function to get hashtags associated with the neutral class from the original training dataframe\nneutral_hashtags = hashtag_extract(train['message'][train['sentiment'] == 0])\n\n# Use the hashtags extract function to get hashtags associated with the news class from the original training dataframe\nnews_hashtags = hashtag_extract(train['message'][train['sentiment'] == 2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a single list for the positive sentiment\npositive_hashtags = sum(positive_hashtags, [])\n\n# Create a single list for the negative sentiment\nnegative_hashtags = sum(negative_hashtags, [])\n\n# Create a single list for the neutral sentiment\nneutral_hashtags = sum(neutral_hashtags, [])\n\n# Create a single list for the news sentiment\nnews_hashtags = sum(news_hashtags, [])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a distribution plot of the most frequent hashtags in the positive hashtags list \nfreq = nltk.FreqDist(positive_hashtags)\n\n# Create a dataframe from the result of the frequency distribution plot, using the hashtags in one column and frequencies\n# in a second\ndf = pd.DataFrame({'Hashtags' : list(freq.keys()),\n                   'Count' : list(freq.values())})\n\n# Sort the hashtags by order of descending counts and show the first 10 rows i.e the 10 most frequent hashtags\ndf_pos = df.sort_values(by = 'Count', ascending = False)\ndf_pos.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bar plot using the group dataframe  to visualise the number of tweets per class\nfig = go.Figure(go.Bar(x = df_pos['Hashtags'].head(10), \n                       y = df_pos['Count'].head(10).sort_values(ascending = False), # Specify x and y variables\n                       marker = {'color': df_pos['Count'],'colorscale': 'viridis'})) # Select a colour for the graph\n\n# Add title, x and y axis labels to the bar chart\nfig.update_layout(yaxis_title = 'Hashtag Counts', xaxis_title = 'Hashtags', \n                  title = 'Count of Hashtags for Positive Man-Made Climate Change Sentiment')\n\n# Show the bar plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a distribution plot of the most frequent hashtags in the negative hashtags list\nfreq = nltk.FreqDist(negative_hashtags)\n\n# Create a dataframe the results of the frequency distribution plot , using the hashtags in the one column and frequencies\n# in a second\ndf = pd.DataFrame({'Hashtags' : list(freq.keys()),\n                   'Count' : list(freq.values())})\n\n# Sort the hashtags by order descending counts and show the first 10 rows i.e the 10 most frequent hashtags\ndf_neg = df.sort_values(by = 'Count', ascending = False)\ndf_neg.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bar plot using the group dataframe  to visualise the number of tweets per class\nfig = go.Figure(go.Bar(x = df_neg['Hashtags'].head(10), \n                       y = df_neg['Count'].head(10).sort_values(ascending = False), # Specify x and y variables\n                       marker = {'color': df_neg['Count'],'colorscale': 'viridis'})) # Select a colour for the graph\n\n# Add title, x and y axis labels to the bar chart\nfig.update_layout(yaxis_title = 'Hashtag Counts', xaxis_title = 'Hashtags', \n                  title = 'Count of Hashtags for Negative Man-Made Climate Change Sentiment')\n\n# Show the bar plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a distribution plot of the most frequent hashtags in neutral hashtags list\nfreq = nltk.FreqDist(neutral_hashtags)\n\n# Create a dataframe the results of the frequency distribution plot, using the hashtags in the one column and frequencies\n# in a second\ndf = pd.DataFrame({'Hashtags' : list(freq.keys()),\n                   'Count' : list(freq.values())})\n# Sort the hashtags by order descending counts and show the first 10 rows i.e the 10 most frequent hashtags\ndf_neu = df.sort_values(by = \"Count\", ascending = False)\ndf_neu.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bar plot using the group dataframe  to visualise the number of tweets per class\nfig = go.Figure(go.Bar(x = df_neu['Hashtags'].head(10), \n                       y = df_neu['Count'].head(10).sort_values(ascending = False), # Specify x and y variables\n                       marker = {'color': df_neu['Count'],'colorscale': 'viridis'})) # Select a colour for the graph\n\n# Add title, x and y axis labels to the bar chart\nfig.update_layout(yaxis_title = 'Hashtag Counts', xaxis_title = 'Hashtags', \n                  title = 'Count of Hashtags for Neutral Man-Made Climate Change Sentiment')\n\n# Show the bar plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a distribution plot of the most frequent hashtags in news hashtags list\nfreq = nltk.FreqDist(news_hashtags)\n\n# Create a dataframe the results of the frequency distribution plot, using the hashtags in the one column and frequencies\n# in a second\ndf = pd.DataFrame({'Hashtags' : list(freq.keys()),\n                   'Count' : list(freq.values())})\n\n# Sort the hashtags by order descending counts and show the first 10 rows i.e the 10 most frequent hashtags\ndf_news = df.sort_values(by = 'Count', ascending = False)\ndf_news.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bar plot using the group dataframe  to visualise the number of tweets per class\nfig = go.Figure(go.Bar(x = df_news['Hashtags'].head(10), \n                       y = df_news['Count'].head(10), # Specify x and y variables\n                       marker = {'color': df_news['Count'],'colorscale': 'viridis'})) # Select a colour for the graph\n\n# Add title, x and y axis labels to the bar chart\nfig.update_layout(yaxis_title = 'Hashtag Counts', xaxis_title = 'Hashtags', \n                  title = 'Count of Hashtags for News Climate Change Sentiment')\n\n# Show the bar plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Words**\n\nLet's visualize the most frequent words in each class in order to get a closer view of words which were most frquently using in each class.","metadata":{}},{"cell_type":"code","source":"# Use the clean, preprocessed training data in order to visualise the most frequently use non-stopword words for each sentiment\n\n# Create function to exclude subject matter words such as climate, change, global, and warming\nextractor = lambda words:  \" \".join([word for word in words.split() if word not in climate_list])\n\n# Extract, from the training dataset, all the tweets written by individuals who were pro man-made climate change\ndata_pos = lemmatized_train[lemmatized_train['sentiment'] == 1] \ndata_pos = data_pos['message'].apply(extractor) # Extract the message column which holds just the tweets in this sentiment\n\n# Extract, from the training dataset, all the tweets written by individuals who were anti man-made climate change\ndata_neg = lemmatized_train[lemmatized_train['sentiment'] == -1]\ndata_neg = data_neg['message'].apply(extractor)# Extract the message column which holds just the tweets in this sentiment\n\n# Extract, from the training dataset, all the tweets written by individuals who were neutral on man-made climate change\ndata_neutral = lemmatized_train[lemmatized_train['sentiment'] == 0]\ndata_neutral = data_neutral['message'].apply(extractor) # Extract the message column which holds just the tweets in this sentiment\n\n# Extract, from the training dataset, all the tweets related to news about climate change\ndata_news = lemmatized_train[lemmatized_train['sentiment'] == 2]\ndata_news = data_news['message'].apply(extractor) # Extract the message column which holds just the tweets in this sentiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the tweets have been extracted for each sentiment, `CountVectorizer` is used to create a bag of words for each sentiment to be used in visualizing the frequencies of each words in the entire document, not including stopwords.\n","metadata":{}},{"cell_type":"code","source":"# Instantiate CountVectorizer as cv_pos\ncv_pos = CountVectorizer()\ndocs_pos = cv_pos.fit_transform(data_pos) # Fit cv_pos to the series containing tweets associated with the positive sentiment\nfeatures_pos = cv_pos.get_feature_names() # Get the words used in the positive sentiment\n\n# Instantiate CountVectorizer as cv_neg\ncv_neg = CountVectorizer()\ndocs_neg = cv_neg.fit_transform(data_neg) # Fit cv_neg to the series containing tweets associated with the negative sentiment\nfeatures_neg = cv_neg.get_feature_names()# Get the words used in the negative sentiment\n\n# Instantiate CountVectorizer as cv_neutral\ncv_neutral = CountVectorizer()\n# Fit cv_neutral to the series containing tweets associated with the neutral sentiment\ndocs_neutral = cv_neutral.fit_transform(data_neutral)  \nfeatures_neutral = cv_neutral.get_feature_names() # Get words used in the neutral sentiment\n\n# Instantiate CountVectorizer as cv_news\ncv_news = CountVectorizer()\ndocs_news = cv_news.fit_transform(data_news) # Fit cv_news to the series containing tweets associated with the news sentiment\nfeatures_news = cv_news.get_feature_names() # Get words used in the negative sentiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the words in each tweet for the four sentiments respectively have been extracted, `FreqDistVisualizer` is used to plot the words against their frequency, sorting them from the most to least frequent. ","metadata":{}},{"cell_type":"code","source":"# Create a frequency distribution plot to display the most frequent words associated with the positive sentiment\nviz_pos = FreqDistVisualizer(features = features_pos, orient = 'v', n = 5, color = 'green',\n                            title = 'Frequency Distribution of Top 10 tokens for Pro sentiment')\n#Enlargen the plot\nvisualizer_pos = RadViz(classes = docs_pos, features = features_pos, size = (1080, 720))\n\n# Fit the vectorized bag of words to the instantiated frequency distribution plot\nviz_pos.fit(docs_pos)\nviz_pos.show() # Display the graph of positive sentiment words\n\n# Create a frequency distribution plot to display the most frequent words associated with the negative sentiment\nviz_neg = FreqDistVisualizer(features = features_neg, orient = 'v', n = 5, color = 'red',\n                            title = 'Frequency Distribution of Top 10 tokens for Anti sentiment')\n# Enlargen the plot\nvisualizer_pos = RadViz(classes = docs_pos, features = features_pos, size = (1080, 720))\n\n# Fit the vectorized bag of words to the instantiated frequency distribution plot\nviz_neg.fit(docs_neg)\nviz_neg.show() # Display the graph of negative sentiment words\n\n# Create a frequency distribution plot to display the most frequent words associated with the neutral sentiment\nviz_neutral = FreqDistVisualizer(features = features_neutral, orient = 'v', n = 5, color = 'yellow',\n                                title = 'Frequency Distribution of Top 10 tokens for Neutral sentiment')\n#Enalargen the plot\nvisualizer_neutral = RadViz(classes = docs_pos, features = features_pos, size = (1080, 720))\n\n# Fit the vectorized bag of words to the instantiated frequency distribution plot\nviz_neutral.fit(docs_neutral)\nviz_neutral.show() # Display the graph of neutral sentiment words\n\n# Create a frequency distribution plot to display the most frequent words associated with the news sentiment\nviz_news = FreqDistVisualizer(features = features_news, orient = 'v', n = 5, color = 'purple',\n                             title = 'Frequency Distribution of Top 10 tokens for News class')\n# Enlargen the plot\nvisualizer_pnews = RadViz(classes = docs_pos, features = features_pos, size = (1080, 720))\n\n# Fit the vectorized bag of words to the instantiated frequency distribution plot\nviz_news.fit(docs_news)\nviz_news.show() # Display the graph of news sentiment words","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n## 6. Feature Extraction\n\n<a id='vect'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"code","source":"# Create a function that cleans the training data and prepares it for modelling\ndef preprocessing(string):\n    \n    \n    # Change the casing in the inputted string to lowercase\n    string = string.lower()\n    \n    # Remove url addresses from the string\n    string = re.sub(r\"http\\S+\", \"\", string)\n    \n    # Instantiate TweetTokenizer with an argument that allows for the stripping of twitter handles\n    tknzr = TweetTokenizer(strip_handles = True)\n    \n    # Tokenize the string using TweetTokenizer in order to remove twitter handles\n    string = tknzr.tokenize(string)\n    \n    # Join the tokenized words together into sentences \n    string = \" \".join(string)\n    \n    # Remove punctuation from the string \n    string = re.sub(r'[^a-z0-9\\s]', '', string)\n    string = re.sub(r'[0-9]+', '', string) # replace numbers or number like words with 'number'\n    \n    # Remove rt from the string\n    message = re.sub(r'^rt', '', string)\n    \n    # Return a new string which has been cleaned of noise\n    return message","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1. CountVectorizer","metadata":{}},{"cell_type":"code","source":"# Create a copy our the training data set, train_data, which will be used to build the models\ntrain_data = train.copy()\n\n# Call the created function preprocessing on train_data dataframe message column in order to clear the tweets of noise \ntrain_data['message']= train_data['message'].apply(preprocessing)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With a clean dataframe, train_data in hand, predictors X and label y need to be defined from the dataframe before X can be fed into the vectorizer for encoding and feature creation","metadata":{}},{"cell_type":"code","source":"# The label, y, is defined as the sentiment column in the dataframe, train_data\ny = train_data.sentiment\n\n# The predictors, X, are defined as the message column in the dataframe, train_data\npredictors = train_data.message \n\n# View the shape of the label and predictors\nprint(predictors.shape) # predictors\nprint(y.shape) # label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally `CountVectorizer` is used to encode the words in the document, thus creating features that can be used to build a model","metadata":{}},{"cell_type":"code","source":"# Instantiate CountVectorizer with the ngrams argument as cv\ncv = CountVectorizer(ngram_range =(1,2))\n\n# Use cv to vectorize the text data in the message columnn of the dataframe, creating a new vector, X\nX = cv.fit_transform(predictors)\n\n# View the shape of vectorized the sparse matrix\nprint(\"The predictors have the shape:\", X.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2. Train Test Split","metadata":{}},{"cell_type":"code","source":"# Train test split is called on the variables X and y to create a training and validation set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data after splitting:","metadata":{}},{"cell_type":"code","source":"# Print the shape of the X training variable, X_train \nprint(f'The X training variable has the shape: {X_train.shape}')\n\n# Print the shape of the y training variable, y_train\nprint(f'The y training variable has the shape: {y_train.shape}')\n\n# Print the shape of the X validation variable, X_val\nprint(f'The X validation variable has the shape: {X_val.shape}')\n\n#Print the shape of the y validation variable, y_val\nprint(f'The y validation variable has the shape: {y_val.shape}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Model Training\n\n<a id='modelling'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"markdown","source":"### 7.1. Logistic Regression Classifier","metadata":{}},{"cell_type":"markdown","source":"**Parameters**","metadata":{}},{"cell_type":"markdown","source":"- **random_state**: Arbitary number that allows for reproducability of results\n- **penalty**: A choice of the type of regularization to use.  'l2' (penalize close to zero) or 'l1' (penalize to zero). l2 if the default.\n- **solver**: Defines which optimization algorithm should be used.\n- **multi_class**: Defines how the model should deal with instances where there are more than two classes. 'ovr' or '           'multinomial'.","metadata":{}},{"cell_type":"code","source":"# Instantiate the Logistic Regression model as logreg\nlogreg = LogisticRegression(multi_class = 'ovr', solver = 'liblinear', random_state = 42) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model to our training data\nlogreg.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the validation set\ny_logreg = logreg.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.1.1. Model Performance","metadata":{}},{"cell_type":"markdown","source":"Model Accuracy Metrics Used\n- Confusion Matrix\n- Classification Report\n- F1_score ","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"markdown","source":"`Confusion matrixes` show the number of observations that were predicted to the four sentiment classes against the number of observations that actually belong to those classes.","metadata":{}},{"cell_type":"code","source":"# Define the labels to be used in the confusion matrix\ntrue_labels = ['true : Anti', 'true : Neutral', 'true : Pro', 'true : News']\npred_labels = ['pred : Anti', 'pred : Neutral', 'pred : Pro', 'pred : News']\ntype_labels = ['-1 : Anti', '0 : Neutral', '1 : Pro', '2 : News']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the confusion matrix dataframe to visualise the number of correctly predicted observations\npd.DataFrame(data = confusion_matrix(y_val, y_logreg), index = true_labels, columns = pred_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification Report** ","metadata":{}},{"cell_type":"markdown","source":"A classification report shows the precision, recall and f1 scores of the model's performance.","metadata":{}},{"cell_type":"code","source":"# Create a classification report from the validation set\nlogreg_report = classification_report(y_val, y_logreg, target_names=type_labels)\n\n# Print out the classification report \nprint(logreg_report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The imbalance in the data is evident in the classification report scores.  ","metadata":{}},{"cell_type":"markdown","source":"**F1 Score**","metadata":{}},{"cell_type":"code","source":"f1_score_logreg = f1_score(y_val,y_logreg,average =\"weighted\") \nprint(f'This is the accuracy for the basic LogisticRegression classifier: {f1_score_logreg}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2. SMOTE Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"# Instantiate SMOTE as sm\nsm = SMOTE(random_state = 42) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the SMOTE model on the training data\nX_res, y_res = sm.fit_resample(X_train, y_train) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have instantiating and fitted and resample the data now whats left is ","metadata":{}},{"cell_type":"code","source":"# Instantiate a Logistic Regression model to use with the resampled data as logreg_smote \nlogreg_smote = LogisticRegression(multi_class = 'ovr', solver = 'liblinear', random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the logistic regression model on the resampled data\nlogreg_smote.fit(X_res, y_res)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the validation set\ny_logreg_smote = logreg_smote.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.1.2. Model Performance ","metadata":{}},{"cell_type":"markdown","source":"Model Accuracy Metrics Used\n- Confusion Matrix\n- Classification Report\n- F1_score ","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"# Create a confusion matrix on the validation data\npd.DataFrame(data = confusion_matrix(y_val, y_logreg_smote), index = true_labels, columns = pred_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classifiction Report** ","metadata":{}},{"cell_type":"code","source":"# classification report on the validation set\nsmote_report = classification_report(y_val, y_logreg_smote, target_names = type_labels)\n\n# Print the classification report\nprint(smote_report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**F1 Score**","metadata":{}},{"cell_type":"code","source":"# Calcuate the f1 score on the validation set\nf1_score_logreg_smote = f1_score(y_val, y_logreg_smote, average = \"weighted\")\n\n# Print the f1 score\nprint(f'This is the accuracy for the basic LogisticRegression classifier: {f1_score_logreg_smote}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparison**","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to show the f1 scores of the basic logistic regression model vs the smote logistic regression model\n\n# Create a list of f1 scores\nf1_scores = [f1_score_logreg, f1_score_logreg_smote]\n\n# Create a list to use as row labels\nmodels = [\"logreg(no smote)\",\"logreg(with smote)\"] \n\n# Create metric column name list\nmetrics = ['f1_score']\n\n# Create the dataframe\nmodel_df = pd.DataFrame(data = f1_scores, index = models, columns = metrics)\n\n# Display the first 5 rows of the new dataframe\nmodel_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Logistic Regression model appears to perform better with without SMOTE.","metadata":{}},{"cell_type":"markdown","source":"###  7.3. Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"# Instantiate the support vector classifier model as svm_clf \nsvm_clf = SVC(C = 10, gamma = 0.01)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on the training data\nsvm_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the validation set\ny_svm_CV = svm_clf.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.2.1. Model Performance","metadata":{}},{"cell_type":"markdown","source":"Model Accuracy Metrics Used\n- Confusion Matrix\n- Classification Report\n- F1_score ","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to display the confusion matrix results\npd.DataFrame(data = confusion_matrix(y_val, y_svm_CV), index = true_labels, columns = pred_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classfication Report** ","metadata":{}},{"cell_type":"code","source":"# Print the classification report the validation set\nprint(classification_report(y_val, y_svm_CV, target_names = type_labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**F1 Score**","metadata":{}},{"cell_type":"code","source":"# Calculate the f1 score on the validation set\nf1_score_svm = f1_score(y_val, y_svm_CV, average = \"weighted\") \n\n# Print the f1 score\nprint(f'This is the accuracy for the basic Support Vector Machine classifier: {f1_score_svm}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.3. SMOTE Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"# Instantiate a support vector classifier model to use on the resampled data\nsvm_clf_CV_smote = SVC(C = 10, gamma = 0.01)\n\n# Fit the model on the resampled data\nsvm_clf_CV_smote.fit(X_res,y_res)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict using the validation set\ny_pred_svm_CV_smote = svm_clf_CV_smote.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.2.1. Model Performance","metadata":{}},{"cell_type":"markdown","source":"Model Accuracy Metrics Used\n- Confusion Matrix\n- Classification Report\n- F1_score ","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to display the confusion matrix results\npd.DataFrame(data = confusion_matrix(y_val, y_pred_svm_CV_smote), index = true_labels, columns = pred_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification Report**","metadata":{}},{"cell_type":"code","source":"# Print the classification report the validation set\nprint(classification_report(y_val, y_pred_svm_CV_smote, target_names = type_labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**F1 Score**","metadata":{}},{"cell_type":"code","source":"# Calculate the f1 score on the validation set\nf1_svm_CV_smote = f1_score(y_val, y_pred_svm_CV_smote, average = \"weighted\") \n\n# Print the f1 score\nprint(f'This is the accuracy for the basic Support Vector Machine classifier: {f1_score_svm}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparison**","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to show the f1 scores of the basic support vector classifier model vs the smote support vector classifier\n# model\n\n# Create a list of f1 scores\nf1_scores = [f1_score_svm, f1_svm_CV_smote]\n\n# Create a list to use as row labels\nmodels = [\"svc(no smote)\",\"svc(with smote)\"] \n\n# Create metric column name list\nmetrics = ['f1_score']\n\n# Create the dataframe\nmodel_df = pd.DataFrame(data = f1_scores, index = models, columns = metrics)\n\n# Display the first 5 rows of the new dataframe\nmodel_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again we see that applying SMOTE to our data seems to have a negative effect on the model performance.","metadata":{}},{"cell_type":"markdown","source":"### 7.3. Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"# Instantiate the naive bayes model as nb\nnb = MultinomialNB() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on the training data\nnb.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the validation data\npred_nb = nb.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.3.1. Model Performance","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"markdown","source":"Model Accuracy Metrics Used\n- Confusion Matrix\n- Classification Report\n- F1_score ","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to display the confusion matrix results\npd.DataFrame(data = confusion_matrix(y_val, pred_nb), index = true_labels, columns = pred_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the classification on the validation set\nprint(classification_report(y_val, pred_nb, target_names = type_labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the f1 score on the validation set\nf1_nb = f1_score(y_val, pred_nb, average = \"weighted\") \n\n# Print the f1 score\nprint(f'This is the accuracy for the Naive Bayes classifier: {f1_score_svm}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.4. SMOTE Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"# Instantiate the naive bayes classifier as nb_smote to use on the resampled data\nnb_smote = MultinomialNB()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on the resampled data\nnb_smote.fit(X_res, y_res)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict of the validation set\npred_nbsmote = nb_smote.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.4.1. Model Performance","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"markdown","source":"Model Accuracy Metrics Used\n- Confusion Matrix\n- Classification Report\n- F1_score ","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to display the confusion matrix results\npd.DataFrame(data = confusion_matrix(y_val, pred_nbsmote), index = true_labels, columns = pred_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the classification report of the validation set\nprint(classification_report(y_val, pred_nbsmote, target_names = type_labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the f1 score on the validation set\nf1_nb_smote = f1_score(y_val, pred_nbsmote, average = \"weighted\") \n\n# Print the f1 score\nprint(f'This is the accuracy for the Naive Bayes classifier: {f1_score_svm}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe to show the f1 scores of the Naive Bayes classifier model vs the smote Naive Bayes classifier\n# model\n\n# Create a list of f1 scores\nf1_scores = [f1_nb, f1_nb_smote]\n\n# Create a list to use as row labels\nmodels = [\"NB (no smote)\",\"NB (with smote)\"] \n\n# Create metric column name list\nmetrics = ['f1_score']\n\n# Create the dataframe\nmodel_df = pd.DataFrame(data = f1_scores, index = models, columns = metrics)\n\n# Display the first 5 rows of the new dataframe\nmodel_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of the three models investigated, it seems that only the Naive Bayes model was postively affected by the implentation of SMOTE.","metadata":{}},{"cell_type":"markdown","source":"# Model Validation","metadata":{}},{"cell_type":"markdown","source":"### K-fold cross validation","metadata":{}},{"cell_type":"code","source":"# define classifiers that we'll apply cross validation to\nclf1 = logreg\nclf2 = svm_clf\nclf3 = nb\n\n# create empty list where we'll append the f1 score's mean and std of each classifier obtained through cross validation\ncross_val = []\n\n# loop through list of classifiers and apply cross_val_score function\nfor clf, label in zip([clf1,clf2,clf3],['Logistic Regresion','SVM', 'Naive Bayes']):\n    print(label)\n    scores = cross_val_score(clf, X, y, cv=5, scoring = 'f1_micro')\n    print(\"f1 score: {:0.4f} (+/- {:0.4f})\".format(scores.mean(), scores.std()))\n    cross_val.append([label, scores.mean(), scores.std() ]) # append the scores to the empty list created above","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert list of cv scores to dataframe\ncross_val = pd.DataFrame(cross_val, columns=['Model', 'CV_Mean', 'CV_Std_Dev'])\ncross_val.set_index('Model', inplace=True) # set index to the name of model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view our dataframe containing cross valiation mean and std\ncross_val","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Test Data\n<a id='tes'></a>\n[Back to table of contents](#toc)","metadata":{}},{"cell_type":"code","source":"# Create a copy of the test to perform noise cleaning and vectorization on\ntest_data = test.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the data cleaning function the test_data dataframe to remove noise in preparation for cleaning\ntest_data['message'] = test_data['message'].apply(preprocessing)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use CountVector in order to encode the words in the clean test dataframe\ntest_cv_trans =  cv.transform(test_data['message'])\n\n# Print out the shape of the newly vectorized dataframe\nprint(\"The shape of the data is:\", test_cv_trans.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.1. Logistic Regression","metadata":{}},{"cell_type":"code","source":"# predict on test_data\ny_pred_logreg_sub = logreg.predict(test_cv_trans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.1.1. Logistic Regression Submission","metadata":{}},{"cell_type":"code","source":"# Extract the tweetid column from test_data to use as the submission file index\ntweetid = test_data['tweetid']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe using the test_data tweetid and and the predicted sentiment\nsubmission_logreg = pd.DataFrame({'tweetid' : tweetid, 'sentiment' : y_pred_logreg_sub})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the last 5 rows of the submission file\nsubmission_logreg.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save dataframe to a csv file for submission\nsubmission_logreg.to_csv(\"gabela_climate_change.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.2. Support Vector","metadata":{}},{"cell_type":"code","source":"# predict on test_data\ny_pred_svm_sub = svm_clf.predict(test_cv_trans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.2.1 Support Vector Submission","metadata":{}},{"cell_type":"code","source":"# Create a dataframe using the test_data tweetid and and the predicted sentiment\nsubmission_svc = pd.DataFrame({'tweetid' : tweetid, 'sentiment' : y_pred_svm_sub})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the last 5 rows of the submission file\nsubmission_svc.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save dataframe to a csv file for submission\nsubmission_svc.to_csv(\"gabela.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.3. Naive Bayes","metadata":{}},{"cell_type":"code","source":"y_pred_nb = nb.predict(test_cv_trans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.3.1. Naive Bayes Submission","metadata":{}},{"cell_type":"code","source":"# Create a dataframe using the test_data tweetid and and the predicted sentiment\nsubmission_nb = pd.DataFrame({'tweetid' : tweetid, 'sentiment' : y_pred_nb})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the last 5 rows of the submission file\nsubmission_nb.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save dataframe to a csv file for submission\nsubmission_nb.to_csv(\"AM3_nb_predictions.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Conclusion","metadata":{}},{"cell_type":"markdown","source":"The Logistic Regression performed well compared to its other counterpart producing an f1_score of 0.74 as well as the accuracy score of 0.75.\n* Reasons:\n * The data was linearly separable even though it was multiclassed. Logistic Regression Performs optimally when classified the data is balanced and when there is two classes and assumes that the data is linearly separable. The use of the one vs rest technique allowed to do the classification more simply","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}